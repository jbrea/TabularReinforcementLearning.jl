<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Policies · Tabular Reinforcement Learning</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/highlightjs/default.css" rel="stylesheet" type="text/css"/><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Tabular Reinforcement Learning</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li class="current"><a class="toctext" href>Policies</a><ul class="internal"><li><a class="toctext" href="#Epsilon-Greedy-Policies-1">Epsilon Greedy Policies</a></li><li><a class="toctext" href="#Softmax-Policies-1">Softmax Policies</a></li></ul></li><li><a class="toctext" href="../mdp/">Environments</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li></ul></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Policies</a></li></ul><a class="edit-page" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/dfbcb1035654c2263ebce256423f6a6f630f5d60/docs/src/policies.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Policies</span><a class="fa fa-bars" href="#"></a></div></header><h2><a class="nav-anchor" id="Epsilon-Greedy-Policies-1" href="#Epsilon-Greedy-Policies-1">Epsilon Greedy Policies</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.EpsilonGreedyPolicy" href="#TabularReinforcementLearning.EpsilonGreedyPolicy"><code>TabularReinforcementLearning.EpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct EpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
	ϵ::Float64</code></pre><p>Chooses the action with the highest value with probability 1 - ϵ and selects an action uniformly random with probability ϵ. For states with actions that where never performed before, the behavior of the <a href="#TabularReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy"><code>VeryOptimisticEpsilonGreedyPolicy</code></a> is followed.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/dfbcb1035654c2263ebce256423f6a6f630f5d60/src/epsilongreedypolicies.jl#L29-L37">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.OptimisticEpsilonGreedyPolicy" href="#TabularReinforcementLearning.OptimisticEpsilonGreedyPolicy"><code>TabularReinforcementLearning.OptimisticEpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct OptimisticEpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
	ϵ::Float64</code></pre><p><a href="#TabularReinforcementLearning.EpsilonGreedyPolicy"><code>EpsilonGreedyPolicy</code></a> that samples uniformly from the actions with the highest Q-value and novel actions in each state where actions are available that where never chosen before. </p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/dfbcb1035654c2263ebce256423f6a6f630f5d60/src/epsilongreedypolicies.jl#L50-L57">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.PesimisticEpsilonGreedyPolicy" href="#TabularReinforcementLearning.PesimisticEpsilonGreedyPolicy"><code>TabularReinforcementLearning.PesimisticEpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct PesimisticEpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
	ϵ::Float64</code></pre><p><a href="#TabularReinforcementLearning.EpsilonGreedyPolicy"><code>EpsilonGreedyPolicy</code></a> that does not handle novel actions differently.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/dfbcb1035654c2263ebce256423f6a6f630f5d60/src/epsilongreedypolicies.jl#L59-L64">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy" href="#TabularReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy"><code>TabularReinforcementLearning.VeryOptimisticEpsilonGreedyPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct VeryOptimisticEpsilonGreedyPolicy &lt;: AbstractEpsilonGreedyPolicy
	ϵ::Float64</code></pre><p><a href="#TabularReinforcementLearning.EpsilonGreedyPolicy"><code>EpsilonGreedyPolicy</code></a> that samples uniformly from novel actions in each state where actions are available that where never chosen before. For Q-value based methods (e.g. <a href="../learners/#TabularReinforcementLearning.QLearning"><code>QLearning</code></a> or <a href="../learners/#TabularReinforcementLearning.SmallBackups"><code>SmallBackups</code></a>) the most optimistic behavior is to choose <code>initvalue</code> = 1/(1 - γ) * rmax where rmax is the maximal value ever returned by the environment.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/dfbcb1035654c2263ebce256423f6a6f630f5d60/src/epsilongreedypolicies.jl#L39-L48">source</a><br/></section><h2><a class="nav-anchor" id="Softmax-Policies-1" href="#Softmax-Policies-1">Softmax Policies</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.SoftmaxPolicy" href="#TabularReinforcementLearning.SoftmaxPolicy"><code>TabularReinforcementLearning.SoftmaxPolicy</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct SoftmaxPolicy &lt;: AbstractSoftmaxPolicy
	β::Float64</code></pre><p>Choose action <span>$a$</span> with probability</p><div>\[\frac{e^{\beta x_a}}{\sum_{a&#39;} e^{\beta x_{a&#39;}}}\]</div><p>where <span>$x$</span> is a vector of values for each action. In states with actions that were never chosen before, a uniform random novel action is returned.</p><pre><code class="language-none">SoftmaxPolicy(; β = 1.)</code></pre><p>Returns a SoftmaxPolicy with default β = 1.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/dfbcb1035654c2263ebce256423f6a6f630f5d60/src/softmaxpolicy.jl#L1-L17">source</a><br/></section><footer><hr/><a class="previous" href="../learners/"><span class="direction">Previous</span><span class="title">Learners</span></a><a class="next" href="../mdp/"><span class="direction">Next</span><span class="title">Environments</span></a></footer></article></body></html>
