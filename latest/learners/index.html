<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Learners · Tabular Reinforcement Learning</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/highlightjs/default.css" rel="stylesheet" type="text/css"/><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Tabular Reinforcement Learning</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li class="current"><a class="toctext" href>Learners</a><ul class="internal"><li><a class="toctext" href="#TD-Learner-1">TD Learner</a></li><li><a class="toctext" href="#Policy-Gradient-Learner-1">Policy Gradient Learner</a></li><li><a class="toctext" href="#N-step-Learner-1">N-step Learner</a></li><li><a class="toctext" href="#Model-Based-Learner-1">Model Based Learner</a></li></ul></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../mdp/">Environments</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li></ul></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Learners</a></li></ul><a class="edit-page" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/docs/src/learners.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Learners</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="learners-1" href="#learners-1">Learners</a></h1><h2><a class="nav-anchor" id="TD-Learner-1" href="#TD-Learner-1">TD Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.ExpectedSarsa" href="#TabularReinforcementLearning.ExpectedSarsa"><code>TabularReinforcementLearning.ExpectedSarsa</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct ExpectedSarsa &lt;: AbstractTDLearner
	α::Float64
	γ::Float64
	unseenvalue::Float64
	params::Array{Float64, 2}
	traces::AbstractTraces
	policy::AbstractPolicy</code></pre><p>Expected Sarsa Learner with learning rate <code>α</code>, discount factor <code>γ</code>,  Q-values <code>params</code> and eligibility <code>traces</code>.</p><p>The Q-values are updated according to <span>$Q(a, s) ← α δ e(a, s)$</span> where <span>$δ = r + γ \sum_{a&#39;} \pi(a&#39;, s&#39;) Q(a&#39;, s&#39;) - Q(a, s)$</span>  with next state <span>$s&#39;$</span>, probability <span>$\pi(a&#39;, s&#39;)$</span> of choosing action <span>$a&#39;$</span> in next state <span>$s&#39;$</span> and <span>$e(a, s)$</span> is the eligibility trace (see <a href="#TabularReinforcementLearning.NoTraces"><code>NoTraces</code></a>,  <a href="#TabularReinforcementLearning.ReplacingTraces"><code>ReplacingTraces</code></a> and <a href="#TabularReinforcementLearning.AccumulatingTraces"><code>AccumulatingTraces</code></a>).</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/tdlearning.jl#L68-L86">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.ExpectedSarsa-Tuple{}" href="#TabularReinforcementLearning.ExpectedSarsa-Tuple{}"><code>TabularReinforcementLearning.ExpectedSarsa</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ExpectedSarsa(; ns = 10, na = 4, α = .1, γ = .9, λ = .8, 
				tracekind = ReplacingTraces, initvalue = Inf64,
				unseenvalue = 0.,
				policy = VeryOptimisticEpsilonGreedyPolicy(.1))</code></pre><p>See also  <a href="#initunseen-1">Initial values, novel actions and unseen values</a>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/tdlearning.jl#L96-L103">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.QLearning" href="#TabularReinforcementLearning.QLearning"><code>TabularReinforcementLearning.QLearning</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct QLearning &lt;: AbstractTDLearner
	α::Float64
	γ::Float64
	unseenvalue::Float64
	params::Array{Float64, 2}
	traces::AbstractTraces</code></pre><p>QLearner with learning rate <code>α</code>, discount factor <code>γ</code>, Q-values <code>params</code> and eligibility <code>traces</code>.</p><p>The Q-values are updated &quot;off-policy&quot; according to <span>$Q(a, s) ← α δ e(a, s)$</span> where <span>$δ = r + γ \max_{a&#39;} Q(a&#39;, s&#39;) - Q(a, s)$</span> with next state <span>$s&#39;$</span> and <span>$e(a, s)$</span> is the eligibility trace (see <a href="#TabularReinforcementLearning.NoTraces"><code>NoTraces</code></a>,  <a href="#TabularReinforcementLearning.ReplacingTraces"><code>ReplacingTraces</code></a> and <a href="#TabularReinforcementLearning.AccumulatingTraces"><code>AccumulatingTraces</code></a>).</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/tdlearning.jl#L21-L37">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.QLearning-Tuple{}" href="#TabularReinforcementLearning.QLearning-Tuple{}"><code>TabularReinforcementLearning.QLearning</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">QLearning(; ns = 10, na = 4, α = .1, γ = .9, λ = .8, 
			tracekind = ReplacingTraces, initvalue = Inf64, unseenvalue = 0.)</code></pre><p>See also  <a href="#initunseen-1">Initial values, novel actions and unseen values</a>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/tdlearning.jl#L38-L43">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.Sarsa" href="#TabularReinforcementLearning.Sarsa"><code>TabularReinforcementLearning.Sarsa</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct Sarsa &lt;: AbstractTDLearner
	α::Float64
	γ::Float64
	unseenvalue::Float64
	params::Array{Float64, 2}
	traces::AbstractTraces</code></pre><p>Sarsa Learner with learning rate <code>α</code>, discount factor <code>γ</code>, Q-values <code>params</code> and eligibility <code>traces</code>.</p><p>The Q-values are updated &quot;on-policy&quot; according to <span>$Q(a, s) ← α δ e(a, s)$</span> where <span>$δ = r + γ Q(a&#39;, s&#39;) - Q(a, s)$</span> with next state <span>$s&#39;$</span>, next action <span>$a&#39;$</span> and <span>$e(a, s)$</span> is the eligibility trace (see <a href="#TabularReinforcementLearning.NoTraces"><code>NoTraces</code></a>,  <a href="#TabularReinforcementLearning.ReplacingTraces"><code>ReplacingTraces</code></a> and <a href="#TabularReinforcementLearning.AccumulatingTraces"><code>AccumulatingTraces</code></a>).</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/tdlearning.jl#L44-L60">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.Sarsa-Tuple{}" href="#TabularReinforcementLearning.Sarsa-Tuple{}"><code>TabularReinforcementLearning.Sarsa</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">Sarsa(; ns = 10, na = 4, α = .1, γ = .9, λ = .8, 
		tracekind = ReplacingTraces, initvalue = Inf64, unseenvalue = 0.)</code></pre><p>See also  <a href="#initunseen-1">Initial values, novel actions and unseen values</a>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/tdlearning.jl#L61-L66">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.AccumulatingTraces" href="#TabularReinforcementLearning.AccumulatingTraces"><code>TabularReinforcementLearning.AccumulatingTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct AccumulatingTraces &lt;: AbstractTraces
	λ::Float64
	γλ::Float64
	trace::Array{Float64, 2}
	minimaltracevalue::Float64</code></pre><p>Decaying traces with factor γλ. </p><p>Traces are updated according to	<span>$e(a, s) ←  1 + e(a, s)$</span> for the current action-state pair and <span>$e(a, s) ←  γλ e(a, s)$</span> for all other pairs unless <span>$e(a, s) &lt;$</span> <code>minimaltracevalue</code> where the trace is set to 0  (for computational efficiency).</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/traces.jl#L41-L54">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.AccumulatingTraces-Tuple{}" href="#TabularReinforcementLearning.AccumulatingTraces-Tuple{}"><code>TabularReinforcementLearning.AccumulatingTraces</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">AccumulatingTraces(ns, na, λ::Float64, γ::Float64; minimaltracevalue = 1e-12)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/traces.jl#L55-L57">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.NoTraces" href="#TabularReinforcementLearning.NoTraces"><code>TabularReinforcementLearning.NoTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct NoTraces &lt;: AbstractTraces</code></pre><p>No eligibility traces, i.e. <span>$e(a, s) = 1$</span> for current action <span>$a$</span> and state <span>$s$</span> and zero otherwise.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/traces.jl#L1-L6">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.ReplacingTraces" href="#TabularReinforcementLearning.ReplacingTraces"><code>TabularReinforcementLearning.ReplacingTraces</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct ReplacingTraces &lt;: AbstractTraces
	λ::Float64
	γλ::Float64
	trace::Array{Float64, 2}
	minimaltracevalue::Float64</code></pre><p>Decaying traces with factor γλ. </p><p>Traces are updated according to	<span>$e(a, s) ←  1$</span> for the current action-state pair and <span>$e(a, s) ←  γλ e(a, s)$</span> for all other pairs unless <span>$e(a, s) &lt;$</span> <code>minimaltracevalue</code> where the trace is set to 0  (for computational efficiency).</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/traces.jl#L24-L37">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.ReplacingTraces-Tuple{}" href="#TabularReinforcementLearning.ReplacingTraces-Tuple{}"><code>TabularReinforcementLearning.ReplacingTraces</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ReplacingTraces(ns, na, λ::Float64, γ::Float64; minimaltracevalue = 1e-12)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/traces.jl#L38-L40">source</a><br/></section><h3><a class="nav-anchor" id="initunseen-1" href="#initunseen-1">Initial values, novel actions and unseen values</a></h3><p>For td-error dependent methods, The exploration-exploitation trade-off depends in the <code>initvalue</code> and the <code>unseenvalue</code>.  To distinguish actions that were never choosen before, i.e. novel actions, the default initial Q-value (field <code>param</code>) is <code>initvalue = Inf64</code>. In a state with novel actions, the <a href="../policies/#policies-1">policy</a> determines how to deal with novel actions. To compute the td-error the <code>unseenvalue</code> is used for states with novel actions.  One way to achieve agressively exploratory behavior is to assure that <code>unseenvalue</code> is larger than the largest possible Q-value.</p><h2><a class="nav-anchor" id="Policy-Gradient-Learner-1" href="#Policy-Gradient-Learner-1">Policy Gradient Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.Critic" href="#TabularReinforcementLearning.Critic"><code>TabularReinforcementLearning.Critic</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct Critic &lt;: AbstractBiasCorrector
	α::Float64
	V::Array{Float64, 1}</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L122-L126">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.Critic-Tuple{}" href="#TabularReinforcementLearning.Critic-Tuple{}"><code>TabularReinforcementLearning.Critic</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">Critic(; α = .1, ns = 10, initvalue = 0.)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L132-L134">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.NoBiasCorrector" href="#TabularReinforcementLearning.NoBiasCorrector"><code>TabularReinforcementLearning.NoBiasCorrector</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct NoBiasCorrector &lt;: AbstractBiasCorrector</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L89-L91">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.PolicyGradientBackward" href="#TabularReinforcementLearning.PolicyGradientBackward"><code>TabularReinforcementLearning.PolicyGradientBackward</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct PolicyGradientBackward &lt;: AbstractPolicyGradient
	α::Float64
	γ::Float64
	params::Array{Float64, 2}
	traces::AccumulatingTraces
	biascorrector::AbstractBiasCorrector</code></pre><p>Policy gradient learning in the backward view.</p><p>The parameters are updated according to <span>$params[a, s] += α * r_{eff} * e[a, s]$</span> where <span>$r_{eff} =  r$</span> for <a href="#TabularReinforcementLearning.NoBiasCorrector"><code>NoBiasCorrector</code></a>, <span>$r_{eff} =  r - rmean$</span> for <a href="#TabularReinforcementLearning.RewardLowpassFilterBiasCorrector"><code>RewardLowpassFilterBiasCorrector</code></a> and e[a, s] is the eligibility trace.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L1-L18">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.PolicyGradientBackward-Tuple{}" href="#TabularReinforcementLearning.PolicyGradientBackward-Tuple{}"><code>TabularReinforcementLearning.PolicyGradientBackward</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">PolicyGradientBackward(; ns = 10, na = 4, α = .1, γ = .9, 
			   tracekind = AccumulatingTraces, initvalue = Inf64,
			   biascorrector = NoBiasCorrector())</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L27-L31">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.PolicyGradientForward" href="#TabularReinforcementLearning.PolicyGradientForward"><code>TabularReinforcementLearning.PolicyGradientForward</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct PolicyGradientForward &lt;: AbstractPolicyGradient
	α::Float64
	γ::Float64
	params::Array{Float64, 2}
	biascorrector::AbstractBiasCorrector</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L41-L47">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.RewardLowpassFilterBiasCorrector" href="#TabularReinforcementLearning.RewardLowpassFilterBiasCorrector"><code>TabularReinforcementLearning.RewardLowpassFilterBiasCorrector</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct RewardLowpassFilterBiasCorrector &lt;: AbstractBiasCorrector
γ::Float64
rmean::Float64</code></pre><p>Filters the reward with factor γ and uses effective reward (r - rmean) to update the parameters.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L96-L103">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.ActorCriticPolicyGradient-Tuple{}" href="#TabularReinforcementLearning.ActorCriticPolicyGradient-Tuple{}"><code>TabularReinforcementLearning.ActorCriticPolicyGradient</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">ActorCriticPolicyGradient(; nsteps = 1, γ = .9, ns = 10, na = 4, 
					        α = .1, αcritic = .1, initvalue = Inf64)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L66-L69">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.EpisodicReinforce-Tuple{}" href="#TabularReinforcementLearning.EpisodicReinforce-Tuple{}"><code>TabularReinforcementLearning.EpisodicReinforce</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">EpisodicReinforce(; kwargs...) = EpisodicLearner(PolicyGradientForward(; kwargs...))</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/policygradientlearning.jl#L60-L62">source</a><br/></section><h2><a class="nav-anchor" id="N-step-Learner-1" href="#N-step-Learner-1">N-step Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.EpisodicLearner" href="#TabularReinforcementLearning.EpisodicLearner"><code>TabularReinforcementLearning.EpisodicLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct EpisodicLearner &lt;: AbstractMultistepLearner
	learner::AbstractReinforcementLearner</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/nsteplearner.jl#L19-L22">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.NstepLearner" href="#TabularReinforcementLearning.NstepLearner"><code>TabularReinforcementLearning.NstepLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct NstepLearner &lt;: AbstractReinforcementLearner
	nsteps::Int64
	learner::AbstractReinforcementLearner</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/nsteplearner.jl#L2-L6">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.NstepLearner-Tuple{}" href="#TabularReinforcementLearning.NstepLearner-Tuple{}"><code>TabularReinforcementLearning.NstepLearner</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">NstepLearner(; nsteps = 10, learner = Sarsa, kwargs...) = 
	NstepLearner(nsteps, learner(; kwargs...))</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/nsteplearner.jl#L11-L14">source</a><br/></section><h2><a class="nav-anchor" id="Model-Based-Learner-1" href="#Model-Based-Learner-1">Model Based Learner</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.SmallBackups" href="#TabularReinforcementLearning.SmallBackups"><code>TabularReinforcementLearning.SmallBackups</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct SmallBackups &lt;: AbstractReinforcementLearner
	γ::Float64
	maxcount::UInt64
	minpriority::Float64
	counter::Int64
	Q::Array{Float64, 2}
	Qprev::Array{Float64, 2}
	V::Array{Float64, 1}
	Nsa::Array{Int64, 2}
	Ns1a0s0::Array{Dict{Tuple{Int64, Int64}, Int64}, 1}
	queue::PriorityQueue</code></pre><p>See <a href="http://proceedings.mlr.press/v28/vanseijen13.html">Harm Van Seijen, Rich Sutton ; Proceedings of the 30th International Conference on Machine Learning, PMLR 28(3):361-369, 2013.</a></p><p><code>maxcount</code> defines the maximal number of backups per action, <code>minpriority</code> is the smallest priority still added to the queue.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/prioritizedsweeping.jl#L1-L18">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.SmallBackups-Tuple{}" href="#TabularReinforcementLearning.SmallBackups-Tuple{}"><code>TabularReinforcementLearning.SmallBackups</code></a> — <span class="docstring-category">Method</span>.</div><div><p>SmallBackups(; ns = 10, na = 4, γ = .9, initvalue = Inf64, maxcount = 3,  				   minpriority = 1e-8)</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/0e5121c88e2b51a7c3eb5cb61fb4fb8a1a268932/src/prioritizedsweeping.jl#L33-L36">source</a><br/></section><footer><hr/><a class="previous" href="../learning/"><span class="direction">Previous</span><span class="title">Learning</span></a><a class="next" href="../policies/"><span class="direction">Next</span><span class="title">Policies</span></a></footer></article></body></html>
