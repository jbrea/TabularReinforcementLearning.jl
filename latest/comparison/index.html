<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Comparison · Tabular Reinforcement Learning</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/highlightjs/default.css" rel="stylesheet" type="text/css"/><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Tabular Reinforcement Learning</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><span class="toctext">Reference</span><ul><li class="current"><a class="toctext" href>Comparison</a><ul class="internal"></ul></li><li><a class="toctext" href="../learning/">Learning</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li><a class="toctext" href="../mdp/">Environments</a></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li></ul></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Comparison</a></li></ul><a class="edit-page" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/faefa88175a9ab682bd65fb65e2d58fdc69bcf6a/docs/src/comparison.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Comparison</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="comparison-1" href="#comparison-1">Comparison Tools</a></h1><p>Since the comparison tools depend on the packages DataFrames and PyPlot they are not loaded automatically. To use them call <code>loadcomparisontools()</code> or individually <code>loadcompare()</code> and <code>loadplotcomparison()</code>.</p><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.compare" href="#TabularReinforcementLearning.compare"><code>TabularReinforcementLearning.compare</code></a> — <span class="docstring-category">Function</span>.</div><div><p>This function is loaded with <code>loadcompare()</code> (requires DataFrames).</p><pre><code class="language-none">compare(N, 
	    environment, 
		metric::AbstractEvaluationMetrics, 
		stoppingcriterion::StoppingCriterion, 
		agent1generator::Function, 
		agent2generator::Function, ...)</code></pre><p>Returns a DataFrame of <code>N</code> runs of all the agents on the <code>environment</code>. </p><p>The DataFrame has the columns :learner (a string identifying the agent), :value (the result of <code>getvalue(metric)</code> and :seed (the random seed used for this run). This macro requires and loads the module DataFrames (can be installed with <code>Pkg.add(&quot;DataFrames&quot;)</code>. If <code>environment</code> is an environment generator function a new environment is generated <code>N</code> times. Otherwise the same environment is reset <code>N</code> times.</p><p><strong>Examples</strong></p><pre><code class="language-none">result = compare(10, () -&gt; MDP(), MeanReward(), ConstantNumberSteps(100), 
				 () -&gt; Agent(QLearning(λ = 0.)), 
				 () -&gt; Agent(QLearning(λ = .8)))</code></pre><p>This can also be written as:</p><pre><code class="language-none">metric = MeanReward()
stopcrit = ConstantNumberSteps(100)
pol = VeryOptimisticEpsilonGreedyPolicy(.1)
getnewmdp() = MDP()
getnewQ1() = Agent(QLearning(λ = 0.), policy = pol)
getnewQ2() = Agent(QLearning(λ = .8), policy = pol)
result = compare(10, getnewmdp, metric, stopcrit, getnewQ1, getnewQ2)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/faefa88175a9ab682bd65fb65e2d58fdc69bcf6a/src/comparisontools.jl#L2-L36">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.plotcomparison" href="#TabularReinforcementLearning.plotcomparison"><code>TabularReinforcementLearning.plotcomparison</code></a> — <span class="docstring-category">Function</span>.</div><div><p>This function is loaded with <code>loadplotcomparison()</code> (requires PyPlot, DataFrames).</p><pre><code class="language-none">plotcomparison(results; labels = Dict(), colors = [], thin = .1, thick = 2, smoothingwindow = 0)</code></pre><p>Plots results obtained with <a href="#TabularReinforcementLearning.compare"><code>compare</code></a>.</p><p>The dictionary <code>labels</code> can be used to rename the legend entries, e.g. <code>labels = Dict(&quot;QLearning_1&quot; =&gt; &quot;QLearning λ = .8&quot;)</code>. The data is smoothed by a moving average of size <code>smoothingwindow</code> (default: no smoothing).</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/tree/faefa88175a9ab682bd65fb65e2d58fdc69bcf6a/src/comparisontools.jl#L39-L49">source</a><br/></section><footer><hr/><a class="previous" href="../usage/"><span class="direction">Previous</span><span class="title">Usage</span></a><a class="next" href="../learning/"><span class="direction">Next</span><span class="title">Learning</span></a></footer></article></body></html>
