<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Environments · Tabular Reinforcement Learning</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Tabular Reinforcement Learning</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Introduction</a></li><li><a class="toctext" href="../usage/">Usage</a></li><li><span class="toctext">Reference</span><ul><li><a class="toctext" href="../comparison/">Comparison</a></li><li><a class="toctext" href="../learning/">Learning</a></li><li><a class="toctext" href="../learners/">Learners</a></li><li><a class="toctext" href="../policies/">Policies</a></li><li class="current"><a class="toctext" href>Environments</a><ul class="internal"><li><a class="toctext" href="#Markov-Decision-Processes-(MDPs)-1">Markov Decision Processes (MDPs)</a></li><li><a class="toctext" href="#Solving-MDPs-1">Solving MDPs</a></li></ul></li><li><a class="toctext" href="../metrics/">Evaluation Metrics</a></li><li><a class="toctext" href="../stop/">Stopping Criteria</a></li><li><a class="toctext" href="../callbacks/">Callbacks</a></li></ul></li><li><a class="toctext" href="../api/">API</a></li></ul></nav><article id="docs"><header><nav><ul><li>Reference</li><li><a href>Environments</a></li></ul><a class="edit-page" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/master/docs/src/mdp.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Environments</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="mdp-1" href="#mdp-1">Environments</a></h1><p>To use other environment, please have a look at the <a href="../api/#api_environments-1">API</a></p><h2><a class="nav-anchor" id="Markov-Decision-Processes-(MDPs)-1" href="#Markov-Decision-Processes-(MDPs)-1">Markov Decision Processes (MDPs)</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.MDP" href="#TabularReinforcementLearning.MDP"><code>TabularReinforcementLearning.MDP</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">mutable struct MDP 
    ns::Int64
    na::Int64
    state::Int64
    trans_probs::Array{AbstractArray, 2}
    reward::Array{Float64, 2}
    initialstates::Array{Int64, 1}
    isterminal::Array{Int64, 1}</code></pre><p>A Markov Decision Process with <code>ns</code> states, <code>na</code> actions, current <code>state</code>, <code>na</code>x<code>ns</code> - array of transition probabilites <code>trans_props</code> which consists for every (action, state) pair of a (potentially sparse) array that sums to 1 (see <a href="#TabularReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}"><code>getprobvecrandom</code></a>, <a href="#TabularReinforcementLearning.getprobvecuniform-Tuple{Any}"><code>getprobvecuniform</code></a>, <a href="#TabularReinforcementLearning.getprobvecdeterministic"><code>getprobvecdeterministic</code></a> for helpers to constract the transition probabilities) <code>na</code>x<code>ns</code> - array of <code>reward</code>, array of initial states <code>initialstates</code>, and <code>ns</code> - array of 0/1 indicating if a state is terminal.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L1-L18">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.MDP-Tuple{Any,Any}" href="#TabularReinforcementLearning.MDP-Tuple{Any,Any}"><code>TabularReinforcementLearning.MDP</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">MDP(ns, na; init = &quot;random&quot;)
MDP(; ns = 10, na = 4, init = &quot;random&quot;)</code></pre><p>Return MDP with <code>init in (&quot;random&quot;, &quot;uniform&quot;, &quot;deterministic&quot;)</code>, where the keyword init determines how to construct the transition probabilites (see also  <a href="#TabularReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}"><code>getprobvecrandom</code></a>, <a href="#TabularReinforcementLearning.getprobvecuniform-Tuple{Any}"><code>getprobvecuniform</code></a>, <a href="#TabularReinforcementLearning.getprobvecdeterministic"><code>getprobvecdeterministic</code></a>).</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L71-L79">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.run!-Tuple{TabularReinforcementLearning.MDP,Array{Int64,1}}" href="#TabularReinforcementLearning.run!-Tuple{TabularReinforcementLearning.MDP,Array{Int64,1}}"><code>TabularReinforcementLearning.run!</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">run!(mdp::MDP, policy::Array{Int64, 1}) = run!(mdp, policy[mdp.state])</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L186-L189">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.run!-Tuple{TabularReinforcementLearning.MDP,Int64}" href="#TabularReinforcementLearning.run!-Tuple{TabularReinforcementLearning.MDP,Int64}"><code>TabularReinforcementLearning.run!</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">run!(mdp::MDP, action::Int64)</code></pre><p>Transition to a new state given <code>action</code>. Returns the new state.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L172-L176">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.setterminalstates!-Tuple{Any,Any}" href="#TabularReinforcementLearning.setterminalstates!-Tuple{Any,Any}"><code>TabularReinforcementLearning.setterminalstates!</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">setterminalstates!(mdp, range)</code></pre><p>Sets <code>mdp.isterminal[range] .= 1</code>, empties the table of transition probabilities for terminal states and sets the reward for all actions in the terminal state to the same value.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L136-L142">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.treeMDP-Tuple{Any,Any}" href="#TabularReinforcementLearning.treeMDP-Tuple{Any,Any}"><code>TabularReinforcementLearning.treeMDP</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">treeMDP(na, depth; init = &quot;random&quot;, branchingfactor = 3)</code></pre><p>Returns a tree structured MDP with na actions and <code>depth</code> of the tree. If <code>init</code> is random, the <code>branchingfactor</code> determines how many possible states a (action, state) pair has. If <code>init = &quot;deterministic&quot;</code> the <code>branchingfactor = na</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L91-L98">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.getprobvecdeterministic" href="#TabularReinforcementLearning.getprobvecdeterministic"><code>TabularReinforcementLearning.getprobvecdeterministic</code></a> — <span class="docstring-category">Function</span>.</div><div><pre><code class="language-none">getprobvecdeterministic(n, min = 1, max = n)</code></pre><p>Returns a <code>SparseVector</code> of length <code>n</code> where one element in <code>min</code>:<code>max</code> has  value 1.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L63-L68">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}" href="#TabularReinforcementLearning.getprobvecrandom-Tuple{Any,Any,Any}"><code>TabularReinforcementLearning.getprobvecrandom</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">getprobvecrandom(n, min, max)</code></pre><p>Returns an array of length <code>n</code> that sums to 1 where all elements outside of <code>min</code>:<code>max</code> are zero.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L51-L56">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.getprobvecrandom-Tuple{Any}" href="#TabularReinforcementLearning.getprobvecrandom-Tuple{Any}"><code>TabularReinforcementLearning.getprobvecrandom</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">getprobvecrandom(n)</code></pre><p>Returns an array of length <code>n</code> that sums to 1. More precisely, the array is a sample of a <a href="https://en.wikipedia.org/wiki/Dirichlet_distribution">Dirichlet distribution</a> with <code>n</code> categories and <span>$lpha_1 = cdots =lpha_n = 1$</span>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L42-L49">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.getprobvecuniform-Tuple{Any}" href="#TabularReinforcementLearning.getprobvecuniform-Tuple{Any}"><code>TabularReinforcementLearning.getprobvecuniform</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">getprobvecuniform(n)  = fill(1/n, n)</code></pre></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdp.jl#L59-L61">source</a></section><h2><a class="nav-anchor" id="Solving-MDPs-1" href="#Solving-MDPs-1">Solving MDPs</a></h2><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.MDPLearner" href="#TabularReinforcementLearning.MDPLearner"><code>TabularReinforcementLearning.MDPLearner</code></a> — <span class="docstring-category">Type</span>.</div><div><pre><code class="language-none">struct MDPLearner
    gamma::Float64
    policy::Array{Int64, 1}
    values::Array{Float64, 1}
    mdp::MDP</code></pre><p>Used to solve <code>mdp</code> with discount factor <code>gamma</code>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdplearner.jl#L1-L9">source</a></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="TabularReinforcementLearning.policy_iteration!-Tuple{TabularReinforcementLearning.MDPLearner}" href="#TabularReinforcementLearning.policy_iteration!-Tuple{TabularReinforcementLearning.MDPLearner}"><code>TabularReinforcementLearning.policy_iteration!</code></a> — <span class="docstring-category">Method</span>.</div><div><pre><code class="language-none">policy_iteration!(mdplearner::MDPLearner)</code></pre><p>Solve MDP with policy iteration using <a href="#TabularReinforcementLearning.MDPLearner"><code>MDPLearner</code></a>.</p></div><a class="source-link" target="_blank" href="https://github.com/jbrea/TabularReinforcementLearning.jl/blob/38a32b419fa0c35fb8aac9a059ef89ba94c632cc/src/mdplearner.jl#L70-L74">source</a></section><footer><hr/><a class="previous" href="../policies/"><span class="direction">Previous</span><span class="title">Policies</span></a><a class="next" href="../metrics/"><span class="direction">Next</span><span class="title">Evaluation Metrics</span></a></footer></article></body></html>
